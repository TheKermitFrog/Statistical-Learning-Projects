---
title: "Brooklyn Airbnb Pricing"
author: "Darren Wang (hsiangw2@illinois.edu)"
date: "`r format(Sys.time(), '%d %B, %Y')`"
output: rmarkdown::github_document
always_allow_html: true
urlcolor: BrickRed
---

```{r, setup, include = FALSE}
knitr::opts_chunk$set(echo = FALSE, fig.align = 'center')
```

```{r, load-packages, include = FALSE}
library(readr)
library(tibble)
library(rsample)
library(dplyr)
library(caret)
library(rpart)
library(rpart.plot)
library(ggplot2)
library(knitr)
library(kableExtra)
library(purrr)
```

***

# Abstract

Online marketplaces are widely used by people nowadays. Airbnb is a popular online marketplace for arranging or offering lodging, primarily homestays, or tourism experiences. Statistical learning methods are used to determine can the rental prices on Airbnb be effectively predicted.

***

# Introduction

Online marketplaces nowadays play an important role in our everyday life. Successful online marketplaces such as Amazon, and eBay create billions of income every year. Airbnb is one of those successful online marketplaces, which provides a platform for hosts to accommodate guests with short-term lodging and tourism-related activities. 

However, how these rental prices are calculated remains a mist to it users. While a house has many attributes that could be used to determine its rental price, there is yet a well recognized standard for rental pricing with these attributes. If we could capture the relationship between a house's attributes and its corresponding rental price, unfair priced items could be identified, thus allowed users of Airbnb to avoid unfair trades. 

Statistical learning techniques were applied to Airbnb listings data in New York, NY during 2019. Rental attributes, and location was used to predict rental prices. The results indicate that this prediction can be made with a small amounts of error when predicting most rentals, but gives higher errors predicting the higher rentals. However, limitations of data and the involvment of subjective judgement in retal pricing suggest the need for further investigation.

***

# Methods

## Data

The data was accessed via Kaggle. [^1] It contains information on Airbnb listings in New York, NY during 2019 including price, rental attributes, and location. For the purposes of this analysis, the data was restricted to short term (one week or less) rentals in Brooklyn that rent for less than $1000 a night. (Additionally, only rentals that have been reviewed are included.)

```{r, load-data, message = FALSE}
airbnb = read_csv(file = "data/AB_NYC_2019.csv")
```

```{r, subset-data}
brooklyn = airbnb %>% 
  filter(minimum_nights <= 7) %>%
  filter(neighbourhood_group == "Brooklyn") %>% 
  filter(number_of_reviews > 0) %>%
  filter(price > 0, price < 1000) %>% 
  na.omit() %>% 
  select(latitude, longitude, room_type, price, minimum_nights, number_of_reviews, 
         reviews_per_month, calculated_host_listings_count, availability_365) %>% 
  mutate(room_type = as.factor(room_type))
```

```{r, split-data}
set.seed(42)
# test-train split
bk_tst_trn_split = initial_split(brooklyn, prop = 0.80)
bk_trn = training(bk_tst_trn_split)
bk_tst = testing(bk_tst_trn_split)
# estimation-validation split
bk_est_val_split = initial_split(bk_trn, prop = 0.80)
bk_est = training(bk_est_val_split)
bk_val = testing(bk_est_val_split)
```

## Modeling

In order to predict the price of rentals, three modeling techniques were considered: linear models, k-nearest neighbors models, and decision tree models. 

- Linear models with and without log transformed responses were considered. Various subsets of predictors, with and without interaction terms were explored.
- k-nearest neighbors models were trained using all available predictor variables. The choice of k was chosen using a validation set.
- Decision tree models were trained using all available predictors. The choice of the complexity parameter was chosen using a validation set.

```{r, linear-models}
# TODO: Fit the following linear models
# 1. a linear model that includes all available predictors
lin = lm(price ~ ., data = bk_est)

# 2. a linear model that selects from all available predictors using backwards selection and AIC
lin_bk = step(lin, direction = "backward", trace = FALSE)

# 3. a linear model that selects from all available predictors, as well as all two-way interactions using backwards selection and AIC
lin_int_bk = step(lm(price ~ (. ^ 2), data = bk_est), direction = "backward", trace = FALSE)

# 4. a linear model that uses a log-transformed response and all available predictors
log_lin = lm(log(price) ~ ., data = bk_est)

# 5. a linear model that uses a log-transformed response and selects from all available predictors using backwards selection and AIC
log_lin_bk = step(log_lin, direction = "backward", trace = FALSE)

# 6. a linear model that uses a log-transformed response and selects from all available predictors, as well as all two-way interactions using backwards selection and AIC
log_lin_int_bk = step(lm(log(price) ~ (. ^ 2), data = bk_est), direction = "backward", trace = FALSE)

# Put them into a list
lin_mods = list(lin, lin_bk, lin_int_bk, log_lin, log_lin_bk, log_lin_int_bk)
names(lin_mods) = c("lin", "lin_bk", "lin_int_bk", "log_lin", "log_lin_bk", "log_lin_int_bk")
```

```{r, knn-models}
k = 1:100
# TODO: Fit KNN models
# use all available predictors
# use the values of k specified above
knn_mods = map(k, ~knnreg(price ~ ., data = bk_est, k = .x))
```

```{r, tree-models}
cp = c(1.000, 0.100, 0.010, 0.001, 0)
# TODO: Fit decision tree models
# use all available predictors
# use the values of cp specified above
# do not adjust minsplit
tree_mods = map(cp, ~rpart(price ~ ., data = bk_est, cp = .x))
```

## Evaluation

To evaluate the ability to predict rental prices, the data was split into estimation, validation, and testing sets. Error metrics and graphics are reported using the validation data in the Results section.

```{r, rmse-functions}
calc_rmse = function(actual, predicted) {
  sqrt(mean( (actual - predicted) ^ 2) )
}

calc_rmse_model = function(model, data, response) {
  actual = data[[response]]
  predicted = predict(model, data)
  sqrt(mean((actual - predicted) ^ 2))
}

calc_rmse_log_model = function(model, data, response) {
  actual = data[[response]]
  predicted = exp(predict(model, data))
  sqrt(mean((actual - predicted) ^ 2))
}
```

***

# Results

```{r, calc-validation-error-lm}
# TODO: calculate validation error for linear models
# Non-log prediction
lin_pred_n = map(lin_mods[1:3], predict, bk_val)

# Log models prediction - map to exp
lin_pred_l = map(map(lin_mods[4:6], predict, bk_val), exp)

# Merge both predictions
lin_preds = c(lin_pred_n, lin_pred_l)

# Calculate validation RMSE for linear models
lin_val_rmse = map_dbl(lin_preds, calc_rmse, actual = bk_val$price)
```

```{r, calc-validation-error-knn}
# TODO: calculate validation error for knn models
knn_preds = map(knn_mods, predict, bk_val)
knn_val_rmse = map_dbl(knn_preds, calc_rmse, actual = bk_val$price)
```

```{r, calc-validation-error-tree}
# TODO: calculate validation error for tree models
tree_preds = map(tree_mods, predict, bk_val)
tree_val_rmse = map_dbl(tree_preds, calc_rmse, actual = bk_val$price)
```

```{r, numeric-results}
# TODO: summarize validation results here
# ouput a human readable table
# at a minimum, report the results for the best linear model, best knn model, and best tree model

# Best Models
best_models_df = tibble("Model" = c("Linear Model with Two-ways Interactions and Backward Selection",
                                    "KNN Model with k = 44",
                                    "Tree Model with cp = 0.001"), 
                        "Validation RMSE" = c(min(lin_val_rmse),
                                              min(knn_val_rmse),
                                              min(tree_val_rmse)))

# Knit kable output
kable(best_models_df, caption = "Table 1: Validation RMSE for Best Models") %>%
  kable_styling(bootstrap_options = "striped", full_width = FALSE)
```

```{r, graphical-results, fig.height = 4, fig.width = 12, fig.cap = "Figure 1: Best Models Actual v.s. Predicted Plots"}
# TODO: create graphical results
# for the best (lm, knn, tree): produce an actual vs predicted plot

pred_act_lin = data.frame(predict_lin = lin_preds[[4]],
                          predict_knn = knn_preds[[44]],
                          predict_tree = tree_preds[[4]],
                          actual = bk_val$price)

p1 = ggplot(data = pred_act_lin, aes(x = predict_lin, y = actual)) + 
  geom_point() + 
  geom_abline(intercept = 0, slope = 1, size = 1.5, color = "navy") + 
  labs(x = "Predicted",
       y = "Actual") +
  theme_minimal() +
  ggtitle("Linear Model") + 
  theme(plot.title = element_text(hjust = 0.5)) 

p2 = ggplot(data = pred_act_lin, aes(x = predict_knn, y = actual)) + 
  geom_point() + 
  geom_abline(intercept = 0, slope = 1, size = 1.5, color = "navy") + 
  labs(x = "Predicted",
       y = "Actual") +
  theme_minimal() +
  ggtitle("KNN Model k = 44") + 
  theme(plot.title = element_text(hjust = 0.5)) 

p3 = ggplot(data = pred_act_lin, aes(x = predict_tree, y = actual)) + 
  geom_point() + 
  geom_abline(intercept = 0, slope = 1, size = 1.5, color = "navy") + 
  labs(x = "Predicted",
       y = "Actual") +
  theme_minimal() +
  ggtitle("Tree Model cp = 0.001") + 
  theme(plot.title = element_text(hjust = 0.5)) 

gridExtra::grid.arrange(p1, p2, p3, ncol = 3)
```

***

# Discussion

```{r, test-rmse}
# Fit chosen model to training data
chosen_mod = step(lm(price ~ (. ^ 2), data = bk_trn), direction = "backward", trace = FALSE)

# Make prediction on test data
chosen_mod_pred = predict(chosen_mod, newdata = bk_tst)

# Calculate test RMSE
chosen_mod_tst_rmse = calc_rmse(predicted = chosen_mod_pred, actual = bk_tst$price)
```

By assessing each model's performance on validation data, the best linear, K nearest neighbour and disicion tree model were found out and display in table 1. It is reasonable to choose the linear model with two-way interactions and using backwards selection with AIC as our final model as it provides the lowest validation error. Using the test data, we got a RMSE of `r chosen_mod_tst_rmse`. Compared to the range of price in this data, ranging from 10 to 999, this model seems to provide reasonable predictions. Even though the fact that 80% of the price are below 160 would makes the model seems unreliable in terms of prediction, the left most plot in Figure 1 tells that the predictions from this model are precise for majority of the data, but are less precise when predicting higher rentals. Thus, our test RMSE is distorted by failures to predict these high rentals.

Empirically, this problem should be alleviated by performing a log transformation on response variable. In fact, our linear model with log transformation on price, two-way interactions and using backwards selection with AIC gave us validation RMSE of `r lin_val_rmse[6]`, which is very similar to our chosen model's `r lin_val_rmse[3]`. Therefore, one possible direction upon improve this model would be transforming response variable. 

In this analysis, we restricted our study to only short term rentals in Brooklyn that rent for less than $1000 a night, the nature of this data limited the possible applications of this model. By using data from only short term rentals in Brooklyn in 2019, it is very likely that the model would fail to predict rentals in another city. Considering only rentals in Brooklyn, the nature of this data would possibly lead to  unreliable result when predicting rentals in a different year. Moreover, we are ignoring the possibility that there are outliers in this data, this might lead to biased result and our fitted models could be drastically affected by potentail outliers. In order to generalized this model, we should include data from more cities, years, and price range. In addition, to avoid the potential harm brought by outliers, exploratory data analysis should come in place before fitting models to data.

We are trying to capture the relationship between attributes of houses and their rental prices by models, which is often a very complicated relationship and a lot of subjective judgements from human are involved. A possible future study is formulating an effective way of reducing the noise brought by subjective thoughts. In addition, macroeconomics factors such as GDP, average income, unemployment rate, and how people react to these factors should play an important role in determining rental prices and therefore worth investigate.

***

# Appendix

## Data Dictionary

- `latitude` - latitude coordinates of the listing
- `longitude` - longitude coordinates of the listing
- `room_type` - listing space type
- `price` - price in dollars
- `minimum_nights` - amount of nights minimum
- `number_of_reviews` - number of reviews
- `reviews_per_month` - number of reviews per month
- `calculated_host_listings_count` - amount of listing per host
- `availability_365` - number of days when listing is available for booking

For additional background on the data, see the data source on Kaggle.

## EDA

```{r, eda-plots, fig.height = 4, fig.width = 12, message = FALSE}
plot_1 = bk_trn %>% 
  ggplot(aes(x = price)) + 
  geom_histogram(bins = 30)

plot_2 = bk_trn %>% 
  ggplot(aes(x = room_type, y = price, colour = price)) + 
  geom_boxplot()

plot_3 = bk_trn %>% 
  ggplot(aes(x = reviews_per_month, y = price)) + 
  geom_point() + geom_smooth(span = 0.3)

gridExtra::grid.arrange(plot_1, plot_2, plot_3, ncol = 3)
```

```{r, price-map, fig.height = 12, fig.width = 12}
bk_trn %>% 
  ggplot(aes(x = longitude, y = latitude, colour = price)) + 
  geom_point()
```

[^1]: [New York City Airbnb Open Data](https://www.kaggle.com/dgomonov/new-york-city-airbnb-open-data)
